import os
import ccxt
import time
import pandas as pd
from datetime import datetime
from Common.CEnum import AUTYPE, DATA_FIELD, KL_TYPE
from Common.CTime import CTime
from Common.func_util import kltype_lt_day, str2float
from KLine.KLine_Unit import CKLine_Unit
from .CommonStockAPI import CCommonStockApi

def GetColumnNameFromFieldList(fileds: str):
    _dict = {
        "time": DATA_FIELD.FIELD_TIME,
        "open": DATA_FIELD.FIELD_OPEN,
        "high": DATA_FIELD.FIELD_HIGH,
        "low": DATA_FIELD.FIELD_LOW,
        "close": DATA_FIELD.FIELD_CLOSE,
        "volume": DATA_FIELD.FIELD_VOLUME,
    }
    return [_dict[x] for x in fileds.split(",")]

class CCXT(CCommonStockApi):
    def __init__(self, code, k_type=KL_TYPE.K_DAY, begin_date=None, end_date=None, autype=AUTYPE.QFQ):
        super(CCXT, self).__init__(code, k_type, begin_date, end_date, autype)

    def get_kl_data(self):
        fields = "time,open,high,low,close,volume"
        
        # === ä»£ç†é…ç½® ===
        my_proxies = {
            'http': 'http://127.0.0.1:10809', 
            'https': 'http://127.0.0.1:10809',
        }
        
        exchange = ccxt.binance({
            'proxies': my_proxies,
            'timeout': 30000,
            'enableRateLimit': True,
        })

        timeframe = self.__convert_type()
        
        # --- ç¼“å­˜æ–‡ä»¶è·¯å¾„ ---
        # ä¾‹å¦‚: BTC_USDT_5m.csv
        safe_code = self.code.replace('/', '_')
        cache_file = f"{safe_code}_{timeframe}.csv"
        
        # 1. è¯»å–æœ¬åœ°ç¼“å­˜
        cached_data = []
        last_timestamp = None
        
        if os.path.exists(cache_file):
            try:
                # è¯»å– CSVï¼Œä¸åŒ…å«è¡¨å¤´ï¼Œåˆ—é¡ºåºï¼štimestamp, open, high, low, close, volume
                df_cache = pd.read_csv(cache_file)
                if not df_cache.empty:
                    # è½¬æ¢ä¸ºåˆ—è¡¨ [ [ts, o, h, l, c, v], ... ]
                    cached_data = df_cache.values.tolist()
                    last_timestamp = int(cached_data[-1][0]) # è·å–æœ€åä¸€æ ¹Kçº¿çš„æ—¶é—´æˆ³
                    print(f"âœ… è¯»å–æœ¬åœ°ç¼“å­˜æˆåŠŸï¼š{len(cached_data)} æ¡ (æœ€æ–°æ—¶é—´: {datetime.fromtimestamp(last_timestamp/1000)})")
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°ä¸‹è½½: {e}")
                cached_data = []

        # 2. å‡†å¤‡ä¸‹è½½
        target_limit = 100000 # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œé¦–æ¬¡ä¸‹è½½çš„æ•°é‡
        new_data = []
        
        # --- è‡ªåŠ¨é‡è¯•ä¸å¢é‡ä¸‹è½½é€»è¾‘ ---
        def fetch_page(since=None, params={}):
            for i in range(3):
                try:
                    if since:
                        return exchange.fetch_ohlcv(self.code, timeframe, since=since, limit=1000, params=params)
                    else:
                        return exchange.fetch_ohlcv(self.code, timeframe, limit=1000, params=params)
                except Exception as e:
                    print(f"ç½‘ç»œæ³¢åŠ¨ï¼Œç¬¬ {i+1} æ¬¡é‡è¯•... ({e})")
                    time.sleep(2)
            raise Exception("è¿æ¥äº¤æ˜“æ‰€å¤±è´¥")

        try:
            if last_timestamp:
                # === å¢é‡æ¨¡å¼ï¼šåªä¸‹è½½æ¯”ç¼“å­˜æ›´æ–°çš„æ•°æ® ===
                print(">> æ­£åœ¨æ£€æŸ¥æ–°æ•°æ®...")
                # since = æœ€åä¸€æ ¹æ—¶é—´ + 1msï¼Œé¿å…é‡å¤
                current_batch = fetch_page(since=last_timestamp + 1)
                while current_batch:
                    new_data.extend(current_batch)
                    print(f"   å·²è·å–æ–°æ•°æ®: {len(new_data)} æ¡")
                    
                    # å¦‚æœå–æ»¡äº†1000æ¡ï¼Œå¯èƒ½è¿˜æœ‰æ›´å¤šï¼Œç»§ç»­å–
                    if len(current_batch) < 1000:
                        break
                        
                    last_ts = current_batch[-1][0]
                    current_batch = fetch_page(since=last_ts + 1)
                    time.sleep(0.1)
            else:
                # === é¦–æ¬¡æ¨¡å¼ï¼šä¸‹è½½æœ€è¿‘çš„ target_limit æ ¹ ===
                print(f">> æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½æœ€è¿‘ {target_limit} æ¡æ•°æ®...")
                current_batch = fetch_page()
                new_data = current_batch
                
                while len(new_data) < target_limit:
                    if not current_batch: break
                    first_ts = current_batch[0][0]
                    params = {'endTime': first_ts - 1}
                    
                    print(f"   åŠ è½½å†å²ä¸­... (å½“å‰ {len(new_data)}/{target_limit})")
                    current_batch = fetch_page(params=params)
                    if not current_batch: break
                    
                    new_data = current_batch + new_data
                    time.sleep(0.1)
                
                # å¦‚æœè¶…å‡ºäº†ï¼Œåªä¿ç•™æœ€å target_limit æ¡
                if len(new_data) > target_limit:
                    new_data = new_data[-target_limit:]

        except Exception as e:
            print(f"âŒ æ•°æ®åŒæ­¥ä¸­æ–­: {e}")
            # å¦‚æœæ˜¯å¢é‡æ›´æ–°å¤±è´¥ï¼Œè‡³å°‘å¯ä»¥ç”¨æ—§ç¼“å­˜è·‘ï¼Œä¸æŠ›å‡ºå¼‚å¸¸
            if not cached_data:
                raise e

        # 3. åˆå¹¶ä¸å»é‡
        if new_data:
            print(f"ğŸ’¾ åˆå¹¶å¹¶ä¿å­˜ {len(new_data)} æ¡æ–°æ•°æ®åˆ°æœ¬åœ°...")
            total_data = cached_data + new_data
            
            # ä½¿ç”¨å­—å…¸å»é‡ (ä»¥æ—¶é—´æˆ³ä¸ºkey)ï¼Œé˜²æ­¢é‡å 
            data_dict = {x[0]: x for x in total_data}
            # æŒ‰æ—¶é—´æ’åº
            sorted_data = sorted(data_dict.values(), key=lambda x: x[0])
            
            # ä¿å­˜å› CSV
            df_save = pd.DataFrame(sorted_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df_save.to_csv(cache_file, index=False)
            
            final_data = sorted_data
        else:
            print(">> æ²¡æœ‰æ–°æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ã€‚")
            final_data = cached_data

        # 4. ç”Ÿæˆ K çº¿å¯¹è±¡è¿”å›ç»™ä¸»ç¨‹åº
        for item in final_data:
            time_obj = datetime.fromtimestamp(item[0] / 1000)
            item_data = [time_obj, item[1], item[2], item[3], item[4], item[5]]
            yield CKLine_Unit(self.create_item_dict(item_data, GetColumnNameFromFieldList(fields)), autofix=True)

    def SetBasciInfo(self): pass
    @classmethod
    def do_init(cls): pass
    @classmethod
    def do_close(cls): pass

    def __convert_type(self):
        _dict = {
            KL_TYPE.K_DAY: '1d', KL_TYPE.K_WEEK: '1w', KL_TYPE.K_MON: '1M',
            KL_TYPE.K_1M: '1m', KL_TYPE.K_5M: '5m', KL_TYPE.K_15M: '15m',
            KL_TYPE.K_30M: '30m', KL_TYPE.K_60M: '1h', 
        }
        return _dict[self.k_type]

    def parse_time_column(self, inp):
        if isinstance(inp, datetime):
            is_day_level = not kltype_lt_day(self.k_type)
            return CTime(inp.year, inp.month, inp.day, inp.hour, inp.minute, auto=is_day_level)
        return inp

    def create_item_dict(self, data, column_name):
        for i in range(len(data)):
            if i == 0: data[i] = self.parse_time_column(data[i])
            else: data[i] = str2float(data[i])
        return dict(zip(column_name, data))